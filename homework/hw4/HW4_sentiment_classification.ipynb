{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MlkRLkCIflYw"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "On3ZVb-ffo8B",
    "outputId": "3ea18b58-cfb3-4585-81c5-25e88efcceba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You should upload the training data and GLoVe Embedding to your Google Drive\n",
    "\n",
    "# Connect this Google Colab to your Google Drive storage (follow the instruction)\n",
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "'''\n",
    "# Commented out because I am running on my own machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztn9Qmax-NmN",
    "outputId": "2d23b66b-3804-4d80-b028-69b9ab32feeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 28 19:34:02 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 466.47       Driver Version: 466.47       CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:65:00.0  On |                  N/A |\n",
      "| 41%   50C    P0    47W / 215W |    784MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1288    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      7272    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      7288    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      7612    C+G   E:\\Wechat\\WeChatApp.exe         N/A      |\n",
      "|    0   N/A  N/A      7972    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A      9944    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9976    C+G   E:\\Wechat\\WeChat.exe            N/A      |\n",
      "|    0   N/A  N/A     10060    C+G   ...8bbwe\\Microsoft.Notes.exe    N/A      |\n",
      "|    0   N/A  N/A     10656    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     11676    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13100    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     14212    C+G   ... Host\\Razer Synapse 3.exe    N/A      |\n",
      "|    0   N/A  N/A     15248    C+G   ...arp.BrowserSubprocess.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check if an Nvidia GPU is available\n",
    "# If not, read this tutorial to enable GPU on Google Colab\n",
    "# https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RraRAKxSiOL9"
   },
   "outputs": [],
   "source": [
    "def load_glove(path, dim=300):\n",
    "    \"\"\"\n",
    "    GLoVe embedding is a way to map a word into a fixed-dimension vector.\n",
    "    This function load the GLoVe embedding\n",
    "    :param path:\n",
    "    :param dim: dimesion of the word vector\n",
    "    :return: a 2D numpy matrix and a dictionary that maps a word into index in the numpy matrix\n",
    "    \"\"\"\n",
    "    matrix = []\n",
    "    word_index = dict()\n",
    "\n",
    "    # Add a zero vector of the same size as \"<PAD>\" token, index of 0\n",
    "    # Add a random vector of the same size as \"<UNK>\" token, index of 1\n",
    "\n",
    "    matrix.append([0.] * dim)\n",
    "    matrix.append([0.] * dim)\n",
    "    word_index['<PAD>'] = 0\n",
    "    word_index['<UNK>'] = 1\n",
    "    # Load from glove\n",
    "    #\n",
    "    with open(path, encoding='latin-1') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            parts = l.split(' ')\n",
    "            vector = [float(x) for x in parts[1:]]\n",
    "            matrix.append(vector)\n",
    "            word_index[parts[0]] = len(word_index)\n",
    "\n",
    "    matrix = np.array(matrix, dtype=np.float)\n",
    "    return matrix, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-p_a0EZNifhT"
   },
   "outputs": [],
   "source": [
    "# Actually call the function to load the GLoVe\n",
    "import os\n",
    "matrix, word_index = load_glove('C:/Users/warre/hw4/glove.6B.50d.txt', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bisBjTStimG7",
    "outputId": "0473eeda-52ce-4e32-f66e-349598323308"
   },
   "outputs": [],
   "source": [
    "# More libraries and download data for the word tokenizer.\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A \"Dataset\" class manage the loading/preprocessing/sampling/packaging of input data\n",
    "# It is used by a \"DataLoader\" in the later part of the code\n",
    "class ImdbDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_file_path, word_index, max_length):\n",
    "        super(ImdbDataset, self).__init__()\n",
    "        self.word_index = word_index\n",
    "        # Paragraph max length\n",
    "        self.ML = max_length\n",
    "        # Load data from data_file_path\n",
    "        self.data = load_json(data_file_path)\n",
    "        # Target is an integer representing a class\n",
    "        # E.g. label=\"positive\" -> target=1\n",
    "        #      label=\"negative\" -> target=0\n",
    "        target_map ={\n",
    "            'positive': 1,\n",
    "            'negative': 0\n",
    "        }\n",
    "        # Tokenize and initialize the target for each data point.\n",
    "        for i, d in enumerate(self.data):\n",
    "          # Tokenize paragraphs into words and punctuations\n",
    "          # Each of the splitted string is called a \"token\"\n",
    "          tokens = word_tokenize(d['text'].lower())\n",
    "\n",
    "          # Indices stores the index of the token in the GLoVe embedding matrix\n",
    "          indices = []\n",
    "          for x in tokens:\n",
    "            if x in word_index:\n",
    "                indices.append(word_index[x])\n",
    "            else:\n",
    "                indices.append(word_index['<UNK>'])\n",
    "          \n",
    "          # Gather everything, and store them into self.data\n",
    "          self.data[i]['token'] = tokens\n",
    "          self.data[i]['indices'] = indices\n",
    "          self.data[i]['target'] = target_map[d['label']]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the length of the dataset, basically, the number of data points\n",
    "        return len(self.data)\n",
    "\n",
    "    def all_targets(self):\n",
    "        # Return all the targets of the dataset, orderly.\n",
    "        return [x['target'] for x in self.data]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        :param idx: an index of a data point from the dataset.\n",
    "        \"\"\"\n",
    "        # Just pick it from self.data\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # Crop the sentence upto a certain length\n",
    "        indices = item['indices'][:self.ML]\n",
    "\n",
    "        # Pad sentence: append <pad_token_index> to the sentence which is shorter than maximum length.\n",
    "        l = len(indices)\n",
    "        if l < self.ML:\n",
    "            indices += [0 for _ in range(self.ML - l)] # 0 is the index of a dummy pad token\n",
    "            # Make sure that the sentence is cropped and padded correctly\n",
    "        assert len(indices) == self.ML\n",
    "        return {\n",
    "            'indices': indices,\n",
    "            'target': item['target']\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def pack(items):\n",
    "        \"\"\"\n",
    "        :param items: list of items, each item is an object returned from __getitem__ function\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Pack item into batch\n",
    "        # Each batch is a dictionary (similar to each item)\n",
    "        batch = {\n",
    "            'indices': torch.LongTensor([x['indices'] for x in items]),\n",
    "            'target': torch.LongTensor([x['target'] for x in items])\n",
    "        }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4IBIGU_alRyf"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# This is a simple version of the CNN for text classification proposed by Yoon Kim\n",
    "# Access the paper here: https://arxiv.org/pdf/1408.5882.pdf\n",
    "class BaseModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_matrix, args):\n",
    "        \"\"\"\n",
    "        :param: embedding_matrix: the GLoVe embedding matrix\n",
    "        :param: args: an object of \"Argument\" class, this class is defined in the later part of the code\n",
    "        \"\"\"\n",
    "\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.device = args.device\n",
    "        hidden_size = args.hidden_size\n",
    "\n",
    "        # create an embedding module\n",
    "        N, D = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding(N, D, _weight=torch.FloatTensor(embedding_matrix))\n",
    "\n",
    "        # Disable gradient update of embedding\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dim = D\n",
    "\n",
    "\n",
    "        # Define the layers\n",
    "        self.conv = nn.Conv1d(D, hidden_size, kernel_size=args.kernel_size)\n",
    "        self.max_pool = nn.MaxPool1d(args.max_length - args.kernel_size + 1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(args.dropout),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(args.dropout),\n",
    "            nn.Linear(args.hidden_size, args.n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # B denotes batch_size\n",
    "        # L denotes sentence length\n",
    "        # D denotes vector dimension\n",
    "\n",
    "        # Get embedding\n",
    "        embedding = self.embedding(batch['indices'].to(self.device))  # size of (B x L x D)\n",
    "        # print('| embedding', tuple(embedding.shape))\n",
    "        x = embedding.transpose(dim0=1, dim1=2)  # B x D x L\n",
    "        # Feed through the neural network\n",
    "        conv_x = self.conv(x)  # B x D x L\n",
    "        # print('| conv_x', tuple(conv_x.shape))\n",
    "\n",
    "        max_pool = self.max_pool(conv_x) # B x D x 1\n",
    "\n",
    "        logits = max_pool.squeeze(dim=2)  # B x D\n",
    "        # print('| logits', tuple(logits.shape))\n",
    "\n",
    "        # Calculate the prediction\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        return logits, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GUCUVKPNmPcT"
   },
   "outputs": [],
   "source": [
    "# Calculate the accuracy score\n",
    "# Do not change this\n",
    "from sklearn.metrics import accuracy_score\n",
    "def metrics(predictions: list, targets: list):\n",
    "    \"\"\"\n",
    "\n",
    "    :param predictions:\n",
    "    :param targets:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return accuracy_score(targets, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TguQuPefmT0T"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_dl, dev_dl, optimizer, args):\n",
    "    \"\"\"\n",
    "    Implementation of stochastic gradient decent\n",
    "    \"\"\"\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    writer = SummaryWriter('./logs/experiment_F1_Adadelta_hs_11_ReLU_ml_1024')\n",
    "    localMax = 0\n",
    "    lowerTime = 0\n",
    "    \n",
    "    for e in range(args.epoch):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_targets, train_preds = [], []\n",
    "        for batch in train_dl:\n",
    "            optimizer.zero_grad()\n",
    "            logits, preds = model(batch)\n",
    "            train_preds += preds.detach().cpu().numpy().tolist()\n",
    "            targets = batch['target'].numpy().tolist()\n",
    "            train_targets += targets\n",
    "            loss = loss_fn(logits, batch['target'].to(args.device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_acc = metrics(train_preds, train_targets)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        dev_targets, dev_preds = [], []\n",
    "        for batch in dev_dl:\n",
    "            s, preds = model(batch)\n",
    "            dev_preds += preds.detach().cpu().numpy().tolist()\n",
    "            targets = batch['target'].numpy().tolist()\n",
    "            dev_targets += targets\n",
    "        dev_acc = metrics(dev_preds, dev_targets)\n",
    "        dev_loss = (1 / len(dev_preds)) * sum(((dev_targets[i] - dev_preds[i]) ** 2) for i in range (len(dev_preds)))\n",
    "        train_loss = (1 / len(train_preds)) * sum(((train_targets[i] - train_preds[i]) ** 2) for i in range (len(train_preds)))\n",
    "        \n",
    "        # Early terminat\n",
    "        \n",
    "        if dev_acc > localMax: \n",
    "            localMax = dev_acc\n",
    "            lowerTime = 0\n",
    "        else:\n",
    "            lowerTime += 1\n",
    "        if lowerTime > 50:\n",
    "            print('Early terminated')\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Logging the epoch and scores\n",
    "        if e % 5 == 0:\n",
    "            print(f'Epoch {e} Train={train_acc:.4f} Dev={dev_acc:.4f} Train_loss={train_loss:.4f} Dev_loss={dev_loss:.4f}')\n",
    "        \n",
    "        writer.add_scalars('acc', {'train': train_acc}, e)\n",
    "        writer.add_scalars('acc', {'dev': dev_acc}, e)\n",
    "        writer.add_scalars('loss', {'train': train_loss}, e)\n",
    "        writer.add_scalars('loss', {'dev': dev_loss}, e)\n",
    "        \n",
    "    writer.close()\n",
    "    print('Max acc: ', localMax)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5jR2n39XmbNy"
   },
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    \"\"\"\n",
    "    Load a json file, return the data\n",
    "    :param path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('Loading', path, end=' ')\n",
    "    with open(path, 'r', encoding='latin-1') as f:\n",
    "        data = json.load(f)\n",
    "    print(len(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-KCQnjAmzfK",
    "outputId": "54e88228-6d55-4335-bb94-d5700403dbdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:/Users/warre/hw4/train.json 25000\n"
     ]
    }
   ],
   "source": [
    "CONST_MAX_LENGTH=1024    # Shorter = faster training, Longer=(possibly) higher accuracy\n",
    "train_dataset = ImdbDataset('C:/Users/warre/hw4/train.json', word_index, CONST_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQVT-oFgm5T7",
    "outputId": "afef6aa6-7546-4d31-ccb0-8f16c49cc0cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:/Users/warre/hw4/dev.json 25000\n"
     ]
    }
   ],
   "source": [
    "dev_dataset = ImdbDataset('C:/Users/warre/hw4/dev.json', word_index, CONST_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5wemwZ36nQBB",
    "outputId": "ac0f465d-8340-49b9-cf05-8d370bf3313e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "# Feel free to define as much as you need\n",
    "# This help the finetuning more organized\n",
    "\n",
    "# ADJUST THESE HYPERPARAMETERS TO COMPLETE THE HOMEWORK\n",
    "\n",
    "class Argument:\n",
    "  n_class= 2    # Number of classes (dont change this)\n",
    "  max_length=CONST_MAX_LENGTH #If you change this, you have to reload the train_dataset and dev_dataset\n",
    "\n",
    "  glove= 'C:/Users/warre/hw4/glove.6B.300d.txt'  # GLoVe embedding version, try all given versions\n",
    "  \n",
    "  # Model arguments\n",
    "  dropout= 0.5          # Dropout rate          Try [0.2:0.8]\n",
    "  hidden_size= 100      # Hidden layer size     Try [64:512]\n",
    "  kernel_size= 11        # CNN kernel size       Try [3,5,7,9,11]\n",
    "\n",
    "  # Training arguments\n",
    "  epoch= 1000           # Number of training epochs.  Try [20:200]\n",
    "  lr= 0.01              # Learning rate               Try [1e-2:1e-4]\n",
    "  batch_size= 50        # Batch size                  Try [32:128]\n",
    "  optimizer= torch.optim.Adadelta        # Optimizer       Try [SGD, Adam, Adadelta]\n",
    "\n",
    "args = Argument\n",
    "\n",
    "# Setup the CUDA device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "args = Argument\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Make sure the cuda is used\n",
    "print('Using device: ', args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZX6C_KSpHa3",
    "outputId": "48c61f94-4e16-40a4-ad58-53480b55dfd5"
   },
   "outputs": [],
   "source": [
    "# Create a dataloader object\n",
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(train_dataset, \n",
    "                      batch_size=args.batch_size, # Mini-batch\n",
    "                      shuffle=True,               # Stochastic \n",
    "                      #num_workers=4,              # 4 external processes dedicated for preprocessing data\n",
    "                      collate_fn=ImdbDataset.pack)# Pack separate samples into a batch\n",
    "dev_dl = DataLoader(dev_dataset, \n",
    "                    batch_size=args.batch_size, \n",
    "                    shuffle=False,                # Don't shuffle in evaluation\n",
    "                    #num_workers=4,\n",
    "                    collate_fn=ImdbDataset.pack)\n",
    "# Load GLoVe embedding\n",
    "embedding_matrix, _ = load_glove(args.glove, dim=300)\n",
    "\n",
    "# Create the model object\n",
    "model = BaseModel(embedding_matrix, args)\n",
    "# Send the model to GPU\n",
    "model.to(args.device)\n",
    "\n",
    "# Select all trainable parameters\n",
    "params = [x for x in model.parameters() if x.requires_grad == True]\n",
    "# Create an optimizer object\n",
    "optimizer = torch.optim.Adadelta(params, lr=args.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "52Oy97vVqG3R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModel(\n",
      "  (embedding): Embedding(400002, 300)\n",
      "  (conv): Conv1d(300, 100, kernel_size=(11,), stride=(1,))\n",
      "  (max_pool): MaxPool1d(kernel_size=1014, stride=1014, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): Linear(in_features=100, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0 Train=0.4960 Dev=0.5004 Train_loss=150.6640 Dev_loss=0.4996\n",
      "Epoch 5 Train=0.6053 Dev=0.5866 Train_loss=0.3947 Dev_loss=0.4134\n",
      "Epoch 10 Train=0.6390 Dev=0.6161 Train_loss=0.3610 Dev_loss=0.3839\n",
      "Epoch 15 Train=0.6697 Dev=0.6488 Train_loss=0.3303 Dev_loss=0.3512\n",
      "Epoch 20 Train=0.7092 Dev=0.6894 Train_loss=0.2908 Dev_loss=0.3106\n",
      "Epoch 25 Train=0.7430 Dev=0.7296 Train_loss=0.2570 Dev_loss=0.2704\n",
      "Epoch 30 Train=0.7713 Dev=0.7574 Train_loss=0.2287 Dev_loss=0.2426\n",
      "Epoch 35 Train=0.7916 Dev=0.7809 Train_loss=0.2084 Dev_loss=0.2191\n",
      "Epoch 40 Train=0.8073 Dev=0.7947 Train_loss=0.1927 Dev_loss=0.2053\n",
      "Epoch 45 Train=0.8211 Dev=0.8058 Train_loss=0.1789 Dev_loss=0.1942\n",
      "Epoch 50 Train=0.8314 Dev=0.8144 Train_loss=0.1686 Dev_loss=0.1856\n",
      "Epoch 55 Train=0.8382 Dev=0.8204 Train_loss=0.1618 Dev_loss=0.1796\n",
      "Epoch 60 Train=0.8442 Dev=0.8228 Train_loss=0.1558 Dev_loss=0.1772\n",
      "Epoch 65 Train=0.8484 Dev=0.8258 Train_loss=0.1516 Dev_loss=0.1742\n",
      "Epoch 70 Train=0.8518 Dev=0.8281 Train_loss=0.1482 Dev_loss=0.1719\n",
      "Epoch 75 Train=0.8556 Dev=0.8298 Train_loss=0.1444 Dev_loss=0.1702\n",
      "Epoch 80 Train=0.8582 Dev=0.8308 Train_loss=0.1418 Dev_loss=0.1692\n",
      "Epoch 85 Train=0.8602 Dev=0.8324 Train_loss=0.1398 Dev_loss=0.1676\n",
      "Epoch 90 Train=0.8621 Dev=0.8332 Train_loss=0.1379 Dev_loss=0.1668\n",
      "Epoch 95 Train=0.8644 Dev=0.8335 Train_loss=0.1356 Dev_loss=0.1665\n",
      "Epoch 100 Train=0.8656 Dev=0.8344 Train_loss=0.1344 Dev_loss=0.1656\n",
      "Epoch 105 Train=0.8668 Dev=0.8349 Train_loss=0.1332 Dev_loss=0.1651\n",
      "Epoch 110 Train=0.8680 Dev=0.8351 Train_loss=0.1320 Dev_loss=0.1649\n",
      "Epoch 115 Train=0.8690 Dev=0.8359 Train_loss=0.1310 Dev_loss=0.1641\n",
      "Epoch 120 Train=0.8705 Dev=0.8361 Train_loss=0.1295 Dev_loss=0.1639\n",
      "Epoch 125 Train=0.8722 Dev=0.8362 Train_loss=0.1278 Dev_loss=0.1638\n",
      "Epoch 130 Train=0.8741 Dev=0.8366 Train_loss=0.1259 Dev_loss=0.1634\n",
      "Epoch 135 Train=0.8750 Dev=0.8370 Train_loss=0.1250 Dev_loss=0.1630\n",
      "Epoch 140 Train=0.8762 Dev=0.8373 Train_loss=0.1238 Dev_loss=0.1627\n",
      "Epoch 145 Train=0.8773 Dev=0.8378 Train_loss=0.1227 Dev_loss=0.1622\n",
      "Epoch 150 Train=0.8788 Dev=0.8380 Train_loss=0.1212 Dev_loss=0.1620\n",
      "Epoch 155 Train=0.8793 Dev=0.8384 Train_loss=0.1207 Dev_loss=0.1616\n",
      "Epoch 160 Train=0.8802 Dev=0.8387 Train_loss=0.1198 Dev_loss=0.1613\n",
      "Epoch 165 Train=0.8809 Dev=0.8393 Train_loss=0.1191 Dev_loss=0.1607\n",
      "Epoch 170 Train=0.8827 Dev=0.8382 Train_loss=0.1173 Dev_loss=0.1618\n",
      "Epoch 175 Train=0.8843 Dev=0.8382 Train_loss=0.1157 Dev_loss=0.1618\n",
      "Epoch 180 Train=0.8841 Dev=0.8394 Train_loss=0.1159 Dev_loss=0.1606\n",
      "Epoch 185 Train=0.8848 Dev=0.8387 Train_loss=0.1152 Dev_loss=0.1613\n",
      "Epoch 190 Train=0.8862 Dev=0.8392 Train_loss=0.1138 Dev_loss=0.1608\n",
      "Epoch 195 Train=0.8870 Dev=0.8390 Train_loss=0.1130 Dev_loss=0.1610\n",
      "Epoch 200 Train=0.8880 Dev=0.8388 Train_loss=0.1120 Dev_loss=0.1612\n",
      "Epoch 205 Train=0.8890 Dev=0.8387 Train_loss=0.1110 Dev_loss=0.1613\n",
      "Epoch 210 Train=0.8898 Dev=0.8390 Train_loss=0.1102 Dev_loss=0.1610\n",
      "Epoch 215 Train=0.8892 Dev=0.8389 Train_loss=0.1108 Dev_loss=0.1611\n",
      "Early terminated\n",
      "Max acc:  0.83988\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Just print out to see the model architecture\n",
    "print(model)\n",
    "# Actual training\n",
    "train_and_evaluate(model, train_dl, dev_dl, optimizer, args)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jw715sYwmN-T"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW4-sentiment-classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
